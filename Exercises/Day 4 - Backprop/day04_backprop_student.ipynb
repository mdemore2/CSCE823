{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 6.4 Backpropagation\n",
    "\n",
    "Goal - Build the components to train an neural network to solve the XOR problem.  Train the ANN and examine the results\n",
    "\n",
    "Students will need to code scections defining the activation functions and the loss function\n",
    "\n",
    "(the program will not run until you do these things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Coding\n",
    "\n",
    "Helper Functions - Activations and derivatives\n",
    "\n",
    "Code the sigmoid and Relu derivatives in the code cell below:\n",
    "\n",
    "```def dSigmoid(x):  # derivative of sigmoid```\n",
    "\n",
    "```def dRelu(z):```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"numpy array-compatible sigmoid (logistic) function\"\"\"\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def dSigmoid(x):  # derivative of sigmoid\n",
    "    \"\"\"returns the derivative of a function\"\"\"\n",
    "    ds = None #placeholder\n",
    "    \n",
    "    #######################     STUDENT CODE      ###################################\n",
    "    # compute the derivative of the sigmoid function\n",
    "    # USE NUMPY to make sure this operation is array compatible\n",
    " \n",
    "    \n",
    "    ####################     END STUDENT CODE      ##################################\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def relu(z):  # rectified linear unit activation\n",
    "    \"\"\"numpy array-compatible ReLU function\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def dRelu(z):\n",
    "    \"\"\" \n",
    "    Derivative of Rectified Linear Unit\n",
    "\n",
    "    \"\"\"\n",
    "    dr = None #placeholder\n",
    "    \n",
    "    #######################     STUDENT CODE      ###################################\n",
    "    # compute the derivative of the ReLU function\n",
    " \n",
    "    \n",
    "    ####################     END STUDENT CODE      ##################################\n",
    "\n",
    "    return dr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Coding:\n",
    "\n",
    "ANN Class definition includes the following function (near the bottom of the cell)\n",
    "\n",
    "```    def compute_loss(self, inputs, desired_targets): ```\n",
    "\n",
    "You will need to code MSE loss for this function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    data = []\n",
    "    layers = []\n",
    "    inputWidth = 1\n",
    "    outputWidth = 1\n",
    "\n",
    "    class Layer:\n",
    "\n",
    "        \"\"\"class defining the elements of an ANN layer\"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            self.w = []\n",
    "            self.b = []\n",
    "            self.nodecount = []\n",
    "            self.activation_fcn = []\n",
    "            self.activation_fcn_derivative = []\n",
    "            self.orderNumber = []\n",
    "            self.previous = None  # link to previous layer\n",
    "            self.next = None  # link to next layer\n",
    "\n",
    "        def set_weights(self, w, b):\n",
    "            \"\"\"set the weights and bias for the layer.  Layer weights should have dimesion: (thislayer_nodecount, previouslayer_nodecount)\n",
    "            the dimension of the bias should be (thislayer_nodecount,1)\"\"\"\n",
    "            self.w = w\n",
    "            self.b = b\n",
    "            return self\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            assert self.w != []\n",
    "            assert self.b != []\n",
    "            self.b = np.random.normal(size=(self.w.shape))  # hidden weight matrix [rows = to, columns = from]\n",
    "            self.w = np.random.normal(size=(self.b.shape))  # hidden biases (column vector)\n",
    "\n",
    "        def set_activation(self, activation_fcn):\n",
    "            \"\"\"Sets the activation function for the entire layer (each layer can have its own activation)\"\"\"\n",
    "            self.activation_fcn = activation_fcn\n",
    "            return self\n",
    "\n",
    "        def set_activation_deriv(self, activation_fcn):\n",
    "            \"\"\"Sets the activation function for the entire layer (each layer can have its own activation)\n",
    "            In python, pass the function with its name only - dont include the parentheses\"\"\"\n",
    "            if activation_fcn == sigmoid:\n",
    "                self.activation_fcn_derivative = copy.deepcopy(dSigmoid)\n",
    "            elif activation_fcn == relu:\n",
    "                self.activation_fcn_derivative = copy.deepcopy(dRelu)\n",
    "            else:\n",
    "                self.activation_fcn_derivative = None\n",
    "\n",
    "        def compute_pre_activation(self, inputs):\n",
    "            \"\"\"Computes the results of the sum of the weights by the inputs and adds the bias\"\"\"\n",
    "            net = np.dot(self.w, inputs) + self.b\n",
    "            return net\n",
    "\n",
    "        def compute_bias_gradient(self, gradient):\n",
    "            \"\"\"Computes the gradient for the bias\"\"\"\n",
    "            gb = np.mean(gradient, axis=1)[:, np.newaxis]  # no regularization\n",
    "            return gb\n",
    "\n",
    "        def compute_weight_gradient(self, inputs, gradient):\n",
    "            \"\"\"Computes the gradient for the weigts at the current input value for this layer\"\"\"\n",
    "            gw = np.dot(gradient, inputs.T)\n",
    "            return gw\n",
    "\n",
    "        def compute_activation(self, net):\n",
    "            return self.activation_fcn(net)\n",
    "\n",
    "        def compute_activation_derivative(self, net):\n",
    "            return self.activation_fcn_derivative(net)\n",
    "\n",
    "        def compute_activation_gradient(self, net, gradient):\n",
    "            \"\"\"Computes the gradient for the activation function at the current net value for this layer\"\"\"\n",
    "            ga = np.multiply(gradient, net)  # Hadamard (elementwise) product\n",
    "            return ga\n",
    "\n",
    "        def compute_forward(self, inputs):\n",
    "            \"\"\"Returns layer ouput from input (shape = [nodeCount, input]) of the weighted input plus bias\n",
    "            input shape must be [lastlayer_nodeCount, samples] or [featurecount, samplecount] \"\"\"\n",
    "            net = self.compute_pre_activation(self, inputs)\n",
    "            layer_out = self.compute_activation(net)\n",
    "            return layer_out\n",
    "\n",
    "        def compute_layer_gradients(self, inputs, net, activation, gradient):\n",
    "            \"\"\" computes the loss gradient with respect to desired output of the layer\n",
    "            a set of desired targets is assumed to be matrix of shape [nodecount, samples]: SGD will have [nodecount,1]\n",
    "            hidden_inputs is assumed to be a matrix of shape [hiddenNodeCount, samples]\n",
    "            returns \n",
    "                g_loss:  the gradient of current layer\n",
    "                g_loss_b:  the gradient of the bias (for update purposes)\n",
    "                    Note:  we dont need to pass the g_loss_w gradient of the weights, but it is computed here\n",
    "                g_loss_backprop: the gradient \n",
    "            \n",
    "            \n",
    "            This follows algorithm 6.4 for the current layer starting inside the For loop\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            # f'(a(k))\n",
    "            d_activation = self.compute_activation_derivative(net)  # derivative of activation:  shape = [NodeCount, samples]\n",
    "            \n",
    "            # g <- g * f'(a(k))\n",
    "            g_loss = self.compute_activation_gradient(d_activation, gradient)  # shape = [NodeCount, samples]  [1, _ ] for outer layer\n",
    "            \n",
    "            # Delta_b(k) J = g (Take the mean across all 4 samples (batch))\n",
    "            g_loss_b = self.compute_bias_gradient(g_loss)  # mean gradient with respect to BIAS, shape = [NodeCount, 1]\n",
    "            \n",
    "            # Delta w(k) J = g * h(k-1)\n",
    "            g_loss_w = self.compute_weight_gradient(activation, g_loss)  # [1, 3]  Hidden layer outputs after activation\n",
    "            \n",
    "            # Determine gradient for the previous layer to pass backwards\n",
    "            # g <- W(k).T * g\n",
    "            g_loss_backprop = np.dot(self.w.T, g_loss)  # gradient to propagate back, shape = [hiddenNodeCount,samples]\n",
    "            \n",
    "            return g_loss_w, g_loss_b, g_loss_backprop\n",
    "\n",
    "        def update_Layer(self, weightUpdate, biasUpdate):\n",
    "            self.w = self.w + weightUpdate\n",
    "            self.b = self.b + biasUpdate\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.layers = []\n",
    "        self.inputWidth = 1\n",
    "        self.outputWidth = 1\n",
    "\n",
    "    def set_input_width(self, inputWidth):\n",
    "        \"\"\"defines the input layer width for the network\"\"\"\n",
    "        self.inputWidth = inputWidth\n",
    "\n",
    "    def add_layer(self, nodecount=1, activation_fcn=relu):\n",
    "        \"\"\"adds a layer to the neural network and returns the layer\n",
    "        Defaults with 1 node using relu activation\"\"\"\n",
    "        \n",
    "        oldLayerCount = len(self.layers)\n",
    "        thislayer = ANN.Layer()\n",
    "        thislayer.orderNumber = oldLayerCount + 1\n",
    "        if oldLayerCount > 0:  # other layers have been added already\n",
    "            lastLayer = self.layers[-1]\n",
    "            thislayer.previous = lastLayer\n",
    "            lastLayer.next = thislayer\n",
    "            layerInputSize = lastLayer.w.shape[1]\n",
    "\n",
    "        else:  # this will be the first layer\n",
    "            layerInputSize = self.inputWidth\n",
    "        thislayer.w = np.zeros((layerInputSize, nodecount))\n",
    "        thislayer.b = np.zeros((1, nodecount))\n",
    "        thislayer.activation_fcn = activation_fcn\n",
    "        thislayer.set_activation_deriv(activation_fcn)\n",
    "        self.layers = self.layers + [thislayer]\n",
    "        return thislayer\n",
    "\n",
    "    def forwardPropagation(self, inputs):\n",
    "        \"\"\"Compute forward pass of two layer network\n",
    "        inputs are assumed to be (shape=[sampleCount,featureCount])\n",
    "        returns a matrix of raw outputs with one row of output per node (shape=[sampleCount, outputNodeCount])\n",
    "        Internal matrices are shaped for efficiency to avoid internal transposes (columns hold observations/samples) \"\"\"\n",
    "\n",
    "        # inputs and outputs will be transposed for efficiency during forwardPropagation and untransposed before returning\n",
    "\n",
    "        nets = []\n",
    "        activations = []\n",
    "        layer_input = inputs.T\n",
    "\n",
    "        for lnum, layer in enumerate(self.layers):\n",
    "            # inputs = inputs + inputs\n",
    "            layer_net = layer.compute_pre_activation(layer_input)\n",
    "            nets.append(layer_net)\n",
    "\n",
    "            layer_out = layer.compute_activation(layer_net)\n",
    "            activations.append(layer_out)\n",
    "\n",
    "            layer_input = layer_out\n",
    "        raw_output = layer_out.T\n",
    "        return raw_output, inputs, nets, activations\n",
    "\n",
    "    def backPropagation(self, inputs, desiredOutputs, learningRate):\n",
    "        w_grads = []\n",
    "        b_grads = []\n",
    "        # store nets and activations for each layer\n",
    "        raw_output, _, nets, activations = self.forwardPropagation(inputs)\n",
    "        layer_desired_out = desiredOutputs\n",
    "\n",
    "        # Note: This is only part of the gradient\n",
    "        layer_grad = desiredOutputs - raw_output\n",
    "\n",
    "        #  computation of full gradient handled inside the loop below\n",
    "        for lnum, layer in reversed(list(enumerate(self.layers))):\n",
    "            if lnum == 0:\n",
    "                prev_layer_output = inputs.T\n",
    "            else:\n",
    "                prev_layer_output = activations[lnum - 1]\n",
    "            if lnum == 1:\n",
    "                w_grad, b_grad, loss_grad = layer.compute_layer_gradients(inputs[lnum], nets[lnum], prev_layer_output,\n",
    "                                                                          layer_grad.T)\n",
    "            else:\n",
    "                w_grad, b_grad, loss_grad = layer.compute_layer_gradients(inputs[lnum], nets[lnum], prev_layer_output,\n",
    "                                                                          layer_grad)\n",
    "            layer.update_Layer(w_grad * learningRate, b_grad * learningRate)\n",
    "            layer_grad = loss_grad\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Compute predictions using forward propagation for single binary classification at threshold\n",
    "        X is a standard dataFrame without biases (shape=[observationCount,featureCount])\n",
    "        returns a standard column vector of binary predictions in {0,1}: (shape=[observationCount, 1])\"\"\"\n",
    "        raw_predictions, net_inputs, net_lst, activation_lst = self.forwardPropagation(X)\n",
    "        preds = raw_predictions > threshold\n",
    "        return preds\n",
    "\n",
    "    def compute_loss(self, inputs, desired_targets):\n",
    "        \"\"\"computes the (scalar) loss using MSE of a set of targets and sigmoid outputs\n",
    "        inputs is assumed to be a matrix of shape [samples, features]\n",
    "         desired_targets is assumed to be a matrix of shape [samples, 1]\"\"\"\n",
    "        \n",
    "        # note that the loss function MSE is on the RAW activation outputs\n",
    "        #  it is not the MSE on the thresholded outputs\n",
    "        \n",
    "        mse = None #placeholder - add code below to compute the MSE loss\n",
    "        \n",
    "        #######################     STUDENT CODE      ###################################\n",
    "        # Complete the MSE loss\n",
    "        \n",
    "       \n",
    "        #####################     END STUDENT CODE   ####################################\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def fit(self, inputs, desired_targets, learningRate, learningRateDecay, tolerance=1e-2, max_epochs = 1000):\n",
    "        done = False\n",
    "        loss_history = []\n",
    "        print(\"Training Model...\")\n",
    "        epoch_counter = 1\n",
    "        while not done:\n",
    "            learningRate = learningRate * learningRateDecay\n",
    "            preds = self.predict(inputs)\n",
    "            correct = desired_targets == preds\n",
    "            curr_loss = self.compute_loss(inputs, desired_targets).item()\n",
    "            loss_history.append(curr_loss)\n",
    "            if curr_loss < tolerance:\n",
    "                done = True\n",
    "            if epoch_counter > max_epochs:\n",
    "                done = True\n",
    "            # run an epoch of backprop\n",
    "            self.backPropagation(inputs, desired_targets, learningRate=learningRate)\n",
    "            epoch_counter+=1\n",
    "        print(\"Training Complete!\")\n",
    "\n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for decision boundary based on threshold exceeding cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDecisionBoundaryBool2(model, featureData, labelData, title, threshold = 0.5):\n",
    "    '''Build decision boundary figrue for 2-input, 1-output boolean logic functions\n",
    "    Note that this assumes a hard sigmoid was used and establishes a cutoff at 0.5\n",
    "    for predicting 0 or 1'''\n",
    "    cutoff = threshold  # Use 0.5 for Sigmoid. 0.0 for TANH\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    # Create filled countour map to color both sides of the boundary\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.c_[X.ravel(), Y.ravel()]\n",
    "    preds = model.predict(grid)  # get predictions\n",
    "    z = preds.reshape(X.shape) > cutoff  # cutoff on predictions to return boolean output\n",
    "    plt.contourf(X, Y, z, cmap='YlOrBr')\n",
    "\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.5)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(txt.item(), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show2dFunctionOutput(model_function, featureData, labelData, title):\n",
    "    \"\"\"display RAW results of arbitrary model function on 2-input (x1,x2) , 1-output (z) graphs\"\"\"\n",
    "    # cutoff = 0.5  # 0.5 for Sigmoid. 0.0 for TANH\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    # Create filled countour map to color both sides of the boundary\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.c_[X.ravel(), Y.ravel()]\n",
    "    outputs, _, _, _ = model_function(grid)  # get predictions\n",
    "    z = outputs.reshape(X.shape)  # reshape predictions for 2d representation\n",
    "    plt.contourf(X, Y, z, cmap='YlOrBr')\n",
    "\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.5)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(txt.item(), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for getting Boolean logic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_data(gate='XOR'):\n",
    "    \"\"\" Two dimensional inputs for logic gates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gate : str\n",
    "        Must be one of these {'AND', 'OR', 'XOR'}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like, shape(samples, features)\n",
    "        Two dim input for logic gates\n",
    "\n",
    "    truth[gate] : array-like, shapes(samples, )\n",
    "        The truth value for this logic gate\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.array([[0., 0.],\n",
    "                  [0., 1.],\n",
    "                  [1., 0.],\n",
    "                  [1., 1.]])\n",
    "\n",
    "    truth = {\n",
    "        'AND': np.array([0, 0, 0, 1]),\n",
    "        'OR': np.array([0, 1, 1, 1]),\n",
    "        'XOR': np.array([0, 1, 1, 0])\n",
    "    }\n",
    "\n",
    "    return X, truth[gate][:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for instantiating, training and evaluating an ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ANN_model(hidden_nodes=3,\n",
    "                    learning_rate=1.0,\n",
    "                    lr_decay=0.999,\n",
    "                    hidden_act=sigmoid,\n",
    "                    output_act=sigmoid,\n",
    "                    gate_type='XOR',\n",
    "                    max_epochs = 1000):\n",
    "    \n",
    "    X, Y = get_input_output_data(gate=gate_type)\n",
    "\n",
    "    model = ANN()\n",
    "    model.set_input_width(X.shape[1])\n",
    "    l1_weights = np.random.rand(hidden_nodes, 2)\n",
    "    l1_bias = np.random.rand(hidden_nodes, 1)\n",
    "    l2_weights = np.random.rand(1, hidden_nodes)\n",
    "    l2_bias = np.random.rand(1, 1)\n",
    "\n",
    "    layer1 = model.add_layer(nodecount=hidden_nodes, activation_fcn=hidden_act)\n",
    "    layer1.set_weights(l1_weights, l1_bias)\n",
    "    layer2 = model.add_layer(nodecount=1, activation_fcn=output_act)\n",
    "    layer2.set_weights(l2_weights, l2_bias)\n",
    "\n",
    "    preds = model.predict(X)\n",
    "    correct = Y == preds\n",
    "\n",
    "    print()\n",
    "    print(\"predictions: \\n\", preds)\n",
    "    print(\"Predictions correct?: \\n\", correct)\n",
    "\n",
    "    loss_history = model.fit(inputs=X, desired_targets=Y, learningRate=learning_rate, learningRateDecay=lr_decay,\n",
    "                             tolerance=1e-1, max_epochs=max_epochs)\n",
    "\n",
    "    preds = model.predict(X)\n",
    "    correct = Y == preds\n",
    "    \n",
    "    print()\n",
    "    print(\"predictions: \\n\", preds)\n",
    "    print(\"Predictions correct?: \\n\", correct)\n",
    "\n",
    "    show2dFunctionOutput(model.forwardPropagation, X, Y, \"Raw Response of Network - student\")\n",
    "    if output_act == sigmoid:\n",
    "        threshold = 0.5 # 0.5 for sigmoid\n",
    "    makeDecisionBoundaryBool2(model, X, Y, \"Thresholded XOR predictions - student\", threshold) \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"Loss (mse)\")\n",
    "    plt.title(\"Loss over iterations (epochs)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate / Train / Eval model on XOR\n",
    "\n",
    "note that you may need to run this several times or incresase the max_epochs for it to solve XOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predictions: \n",
      " [[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "Predictions correct?: \n",
      " [[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "Training Model...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0a1383688734>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                \u001b[0moutput_act\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                \u001b[0mgate_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'XOR'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                max_epochs = 1000)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-1f59cd7011a4>\u001b[0m in \u001b[0;36mtrain_ANN_model\u001b[1;34m(hidden_nodes, learning_rate, lr_decay, hidden_act, output_act, gate_type, max_epochs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     loss_history = model.fit(inputs=X, desired_targets=Y, learningRate=learning_rate, learningRateDecay=lr_decay,\n\u001b[1;32m---> 31\u001b[1;33m                              tolerance=1e-1, max_epochs=max_epochs)\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a1f30d1b47a5>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, inputs, desired_targets, learningRate, learningRateDecay, tolerance, max_epochs)\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdesired_targets\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m             \u001b[0mcurr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m             \u001b[0mloss_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurr_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "#note that you may need to run this several times or incresase the max_epochs for it to solve XOR\n",
    "\n",
    "#note that relu doesnt work very well for the hidden layer... stick with sigmoid\n",
    "\n",
    "train_ANN_model(hidden_nodes=3,\n",
    "               learning_rate=1.0,\n",
    "               lr_decay=0.999,\n",
    "               hidden_act=sigmoid,\n",
    "               output_act=sigmoid,\n",
    "               gate_type='XOR',\n",
    "               max_epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
